package main

import (
	"bytes"
	"crypto/sha256"
	"encoding/gob"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"math/rand"
	"net/http"
	"os"
	"sort"
	"strconv"
	"sync"
	"text/template"
	"time"

	"github.com/gorilla/mux"
)

// --- Constantes et Configuration ---
const (
	indexFilePath         = "index.idx"
	chunkSize             = 8 * 1024 * 1024     // 8 MB
	requiredReplicas      = 2
	minFreeSpaceForUpload = 100 * 1024 * 1024 // 100 MB minimum
	gcJournalPath         = "gc.journal.tmp"    // Chemin pour le fichier de journal du GC
)

// --- Structures de Donn√©es ---

type ChunkCopy struct {
	VolumeName string
	Offset     uint64
	Size       uint32
	Deleted    bool `json:"deleted,omitempty"` // Marqu√© pour suppression
}

type ChunkMetadata struct {
	ChunkIdx int
	Copies   []*ChunkCopy
	Checksum string
}

type FileMetadata struct {
	FileName   string
	TotalSize  int64
	UploadDate time.Time
	Chunks     []*ChunkMetadata
	Status     string `json:"-"`
	Deleted    bool   `json:"deleted,omitempty"` // Marqu√© pour suppression
}

func (fm *FileMetadata) TotalSizeMB() float64 {
	return float64(fm.TotalSize) / (1024 * 1024)
}

type Volume struct {
	Name       string
	DiskID     string
	Address    string
	TotalSpace uint64
	FreeSpace  uint64
	LastSeen   time.Time `json:"-"`
	Status     string
	Type       string `json:"type,omitempty"`
}

func (v *Volume) FreeSpaceGB() float64 {
	return float64(v.FreeSpace) / (1024 * 1024 * 1024)
}

type GlobalState struct {
	sync.RWMutex
	FileIndex         map[string]*FileMetadata
	RegisteredVolumes map[string]*Volume
}

// Structure pour le journal du Garbage Collection
type GCJournal struct {
	// Fait le lien entre un nom de volume et sa map d'offsets (ancien offset -> nouvel offset)
	VolumeOffsetMaps map[string]map[uint64]uint64
}

var state = GlobalState{
	FileIndex:         make(map[string]*FileMetadata),
	RegisteredVolumes: make(map[string]*Volume),
}
var webTemplate *template.Template

// --- Fonctions Principales ---
func main() {
	rand.Seed(time.Now().UnixNano())
	loadIndex()
	replayGCJournalIfExists() // On v√©rifie s'il faut reprendre une op√©ration de GC interrompue

	go cleanupInactiveVolumes()
	go garbageCollectionWorker()

	go func() {
		ticker := time.NewTicker(10 * time.Minute) // Audit toutes les 10 minutes
		defer ticker.Stop()
		for range ticker.C {
			auditDataIntegrity()
		}
	}()

	var err error
	webTemplate, err = template.New("webui").Parse(htmlTemplate)
	if err != nil {
		log.Fatalf("Impossible de parser le template HTML: %v", err)
	}

	r := mux.NewRouter()
	r.HandleFunc("/api/disk/register", registerVolumeHandler).Methods("POST")
	r.HandleFunc("/api/files/upload", uploadFileHandler).Methods("POST")
	r.HandleFunc("/api/files/download/{filename}", downloadFileHandler).Methods("GET")
	r.HandleFunc("/api/files/delete/{filename}", deleteFileHandler).Methods("POST")
	r.HandleFunc("/api/repair", repairHandler).Methods("POST")
	r.HandleFunc("/api/cleanup_replicas", cleanupReplicasHandler).Methods("POST") // NOUVELLE ROUTE
	r.HandleFunc("/api/gc", garbageCollectionHandler).Methods("POST")
	r.HandleFunc("/", webUIHandler).Methods("GET")

	log.Println("Serveur de stockage d√©marr√© sur http://localhost:8080")
	if err := http.ListenAndServe(":8080", r); err != nil {
		log.Fatalf("Le serveur n'a pas pu d√©marrer: %v", err)
	}
}

// --- Logique M√©tier ---
func loadIndex() {
	state.Lock()
	defer state.Unlock()
	file, err := os.Open(indexFilePath)
	if err != nil {
		if os.IsNotExist(err) {
			log.Printf("Fichier d'index '%s' non trouv√©. Un nouveau sera cr√©√©.", indexFilePath)
			return
		}
		log.Fatalf("Erreur √† l'ouverture du fichier d'index: %v", err)
	}
	defer file.Close()
	decoder := gob.NewDecoder(file)
	if err := decoder.Decode(&state.FileIndex); err != nil {
		log.Printf("Erreur au d√©codage de l'index: %v. L'index sera r√©initialis√©.", err)
		state.FileIndex = make(map[string]*FileMetadata)
	} else {
		log.Printf("Index charg√©. %d fichiers index√©s.", len(state.FileIndex))
	}
}

func saveIndex() {
	state.RLock()
	indexCopy := make(map[string]*FileMetadata)
	for k, v := range state.FileIndex {
		indexCopy[k] = v
	}
	state.RUnlock()

	tempPath := indexFilePath + ".tmp"
	file, err := os.Create(tempPath)
	if err != nil {
		log.Printf("ERREUR: Impossible de cr√©er le fichier temporaire pour l'index: %v", err)
		return
	}

	encoder := gob.NewEncoder(file)
	err = encoder.Encode(&indexCopy)
	file.Close()

	if err != nil {
		log.Printf("ERREUR: Impossible d'encoder l'index avec gob: %v", err)
		os.Remove(tempPath)
		return
	}

	if err := os.Rename(tempPath, indexFilePath); err != nil {
		log.Printf("ERREUR: Impossible de finaliser la sauvegarde de l'index: %v", err)
		os.Remove(tempPath)
		return
	}

	log.Printf("Index sauvegard√© de mani√®re atomique avec %d fichiers", len(indexCopy))
}

// Fonction pour rejouer le journal de GC si le serveur a crash√©
func replayGCJournalIfExists() {
	file, err := os.Open(gcJournalPath)
	if err != nil {
		if os.IsNotExist(err) {
			// Le journal n'existe pas, c'est le cas normal.
			return
		}
		log.Fatalf("ERREUR CRITIQUE: Impossible d'ouvrir le journal de GC existant: %v", err)
	}
	defer file.Close()

	log.Println("ATTENTION: Journal de Garbage Collection trouv√©. D√©marrage de la proc√©dure de r√©cup√©ration...")

	var journalData GCJournal
	decoder := gob.NewDecoder(file)
	if err := decoder.Decode(&journalData); err != nil {
		log.Fatalf("ERREUR CRITIQUE: Impossible de d√©coder le journal de GC. Intervention manuelle requise. Fichier: %s. Erreur: %v", gcJournalPath, err)
	}
	file.Close()

	// Appliquer les changements du journal √† l'index en m√©moire
	applyOffsetMaps(&journalData)

	// Sauvegarder l'index maintenant corrig√©
	log.Println("Sauvegarde de l'index r√©cup√©r√©...")
	saveIndex()

	// Supprimer le journal puisque la r√©cup√©ration est termin√©e
	if err := os.Remove(gcJournalPath); err != nil {
		log.Printf("ERREUR CRITIQUE: Impossible de supprimer le journal de GC apr√®s r√©cup√©ration: %v. Veuillez le supprimer manuellement.", err)
	} else {
		log.Println("R√©cup√©ration depuis le journal de GC termin√©e avec succ√®s.")
	}
}

// Fonction pour appliquer les maps d'offset (factoris√©e pour √™tre utilis√©e par le GC et la r√©cup√©ration)
func applyOffsetMaps(journalData *GCJournal) {
	state.Lock()
	defer state.Unlock()

	log.Println("Application des mises √† jour d'offset depuis le journal de GC...")
	for volumeName, offsetMap := range journalData.VolumeOffsetMaps {
		if len(offsetMap) == 0 {
			continue
		}
		log.Printf("Mise √† jour des offsets pour le volume '%s'...", volumeName)
		for _, fileMeta := range state.FileIndex {
			if fileMeta.Deleted {
				continue
			}
			for _, chunkMeta := range fileMeta.Chunks {
				for _, copyInfo := range chunkMeta.Copies {
					if copyInfo.VolumeName == volumeName {
						if newOffset, ok := findNewOffset(copyInfo.Offset, offsetMap); ok {
							if newOffset != copyInfo.Offset {
								log.Printf("Offset chang√© pour chunk (fichier: %s) sur %s: %d -> %d", fileMeta.FileName, volumeName, copyInfo.Offset, newOffset)
								copyInfo.Offset = newOffset
							}
						}
					}
				}
			}
		}
	}
	log.Println("Application des mises √† jour d'offset termin√©e.")
}

func verifyVolumeOnline(volume *Volume) bool {
	if volume.Status != "En ligne" {
		return false
	}

	client := &http.Client{Timeout: 3 * time.Second}
	testURL := fmt.Sprintf("http://%s/health", volume.Address)

	resp, err := client.Get(testURL)
	if err != nil {
		log.Printf("S√âCURIT√â: Volume '%s' d√©clar√© en ligne mais inaccessible: %v", volume.Name, err)
		state.Lock()
		if vol, exists := state.RegisteredVolumes[volume.Name]; exists {
			vol.Status = "Hors ligne"
			log.Printf("S√âCURIT√â: Volume '%s' marqu√© automatiquement hors ligne", volume.Name)
		}
		state.Unlock()
		return false
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		log.Printf("S√âCURIT√â: Volume '%s' r√©pond avec un statut non-OK: %d", volume.Name, resp.StatusCode)
		state.Lock()
		if vol, exists := state.RegisteredVolumes[volume.Name]; exists {
			vol.Status = "Hors ligne"
		}
		state.Unlock()
		return false
	}

	return true
}

func selectVolumesForReplication(count int, excludeDisks []string, requiredSpace uint64) ([]*Volume, error) {
	state.RLock()
	var candidateVolumes []*Volume
	for _, v := range state.RegisteredVolumes {
		if v.Status == "En ligne" && v.FreeSpace >= requiredSpace {
			candidateVolumes = append(candidateVolumes, v)
		}
	}
	state.RUnlock()

	var eligibleVolumes []*Volume
	for _, v := range candidateVolumes {
		if verifyVolumeOnline(v) {
			eligibleVolumes = append(eligibleVolumes, v)
		}
	}

	volumesByDiskID := make(map[string][]*Volume)
	for _, v := range eligibleVolumes {
		isExcluded := false
		for _, excluded := range excludeDisks {
			if v.DiskID == excluded {
				isExcluded = true
				break
			}
		}
		if !isExcluded {
			volumesByDiskID[v.DiskID] = append(volumesByDiskID[v.DiskID], v)
		}
	}

	if len(volumesByDiskID) < count {
		return nil, fmt.Errorf("pas assez de disques physiques V√âRIFI√âS avec l'espace libre requis (%d MB) disponibles (requis: %d, dispo: %d)",
			requiredSpace/(1024*1024), count, len(volumesByDiskID))
	}

	var availableDiskIDs []string
	for diskID := range volumesByDiskID {
		availableDiskIDs = append(availableDiskIDs, diskID)
	}
	rand.Shuffle(len(availableDiskIDs), func(i, j int) {
		availableDiskIDs[i], availableDiskIDs[j] = availableDiskIDs[j], availableDiskIDs[i]
	})

	selectedDisks := availableDiskIDs[:count]
	var result []*Volume
	for _, diskID := range selectedDisks {
		volumesOnThisDisk := volumesByDiskID[diskID]
		selectedVolume := volumesOnThisDisk[0]
		for _, vol := range volumesOnThisDisk {
			if vol.FreeSpace > selectedVolume.FreeSpace {
				selectedVolume = vol
			}
		}
		result = append(result, selectedVolume)
	}

	return result, nil
}

// MODIFI√â: Ajout de la d√©tection de sur-r√©plication
func calculateFileStatus(meta *FileMetadata) string {
	if meta.Deleted {
		return "Supprim√©"
	}

	allChunksProtected := true
	hasUnavailableChunk := false
	hasOverReplicatedChunk := false // Flag pour la sur-r√©plication

	state.RLock()
	defer state.RUnlock()

	for _, chunk := range meta.Chunks {
		onlineCopies := 0
		uniqueDisks := make(map[string]bool)

		for _, copyInfo := range chunk.Copies {
			if copyInfo.Deleted {
				continue
			}

			if vol, exists := state.RegisteredVolumes[copyInfo.VolumeName]; exists && vol.Status == "En ligne" {
				onlineCopies++
				uniqueDisks[vol.DiskID] = true
			}
		}
		
		if onlineCopies > requiredReplicas {
			hasOverReplicatedChunk = true
		}

		if onlineCopies == 0 {
			hasUnavailableChunk = true
			break 
		}

		if onlineCopies < requiredReplicas || len(uniqueDisks) < requiredReplicas {
			allChunksProtected = false
		}
	}

	if hasUnavailableChunk {
		return "Indisponible"
	}

	if !allChunksProtected {
		return "D√©grad√©"
	}
	
	if hasOverReplicatedChunk {
		return "Sur-prot√©g√©"
	}

	return "Prot√©g√©"
}

func verifyChunkExistsOnVolume(volume *Volume, copyInfo *ChunkCopy, expectedChecksum string) bool {
	client := &http.Client{Timeout: 5 * time.Second}
	checkURL := fmt.Sprintf("http://%s/verify_chunk?offset=%d&size=%d&checksum=%s",
		volume.Address, copyInfo.Offset, copyInfo.Size, expectedChecksum)

	resp, err := client.Get(checkURL)
	if err != nil {
		log.Printf("ERREUR VERIFICATION: Impossible de v√©rifier chunk sur '%s': %v", volume.Name, err)
		return false
	}
	defer resp.Body.Close()

	return resp.StatusCode == http.StatusOK
}

func validateUploadedFile(fileName string) error {
	state.RLock()
	meta, exists := state.FileIndex[fileName]
	if !exists {
		state.RUnlock()
		return fmt.Errorf("fichier '%s' non trouv√© dans l'index", fileName)
	}

	chunks := make([]*ChunkMetadata, len(meta.Chunks))
	copy(chunks, meta.Chunks)
	state.RUnlock()

	for i, chunk := range chunks {
		onlineCopies := 0
		uniqueDisks := make(map[string]bool)

		state.RLock()
		for _, copyInfo := range chunk.Copies {
			if copyInfo.Deleted {
				continue
			}

			if vol, exists := state.RegisteredVolumes[copyInfo.VolumeName]; exists && vol.Status == "En ligne" {
				if verifyChunkExistsOnVolume(vol, copyInfo, chunk.Checksum) {
					onlineCopies++
					uniqueDisks[vol.DiskID] = true
				} else {
					log.Printf("ERREUR VALIDATION: Chunk %d du fichier '%s' introuvable sur volume '%s'",
						i, fileName, vol.Name)
				}
			}
		}
		state.RUnlock()

		if onlineCopies < requiredReplicas || len(uniqueDisks) < requiredReplicas {
			return fmt.Errorf("chunk %d du fichier '%s' n'a que %d copies sur %d disques diff√©rents (requis: %d)",
				i, fileName, onlineCopies, len(uniqueDisks), requiredReplicas)
		}
	}

	return nil
}

func auditDataIntegrity() {
	log.Println("D√©but de l'audit d'int√©grit√© des donn√©es...")

	state.RLock()
	filesToAudit := make(map[string]*FileMetadata)
	for filename, meta := range state.FileIndex {
		if !meta.Deleted {
			chunks := make([]*ChunkMetadata, len(meta.Chunks))
			copy(chunks, meta.Chunks)
			filesToAudit[filename] = &FileMetadata{
				FileName:   meta.FileName,
				TotalSize:  meta.TotalSize,
				UploadDate: meta.UploadDate,
				Chunks:     chunks,
			}
		}
	}
	state.RUnlock()

	issuesFound := 0
	for filename, meta := range filesToAudit {
		realStatus := calculateFileStatus(meta)

		if realStatus == "D√©grad√©" || realStatus == "Indisponible" {
			log.Printf("AUDIT: Fichier '%s' a un statut probl√©matique: %s", filename, realStatus)
			issuesFound++

			state.Lock()
			if indexMeta, exists := state.FileIndex[filename]; exists {
				indexMeta.Status = realStatus
			}
			state.Unlock()
		}
	}

	if issuesFound > 0 {
		saveIndex()
		log.Printf("AUDIT: %d probl√®mes d'int√©grit√© d√©tect√©s et corrig√©s", issuesFound)
	} else {
		log.Println("AUDIT: Aucun probl√®me d'int√©grit√© d√©tect√©")
	}
}

func cleanupInactiveVolumes() {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()
	for range ticker.C {
		state.Lock()
		for _, volume := range state.RegisteredVolumes {
			if time.Since(volume.LastSeen) > 45*time.Second && volume.Status == "En ligne" {
				log.Printf("Volume '%s' inactif. Marquage comme Hors ligne.", volume.Name)
				volume.Status = "Hors ligne"
			}
		}
		state.Unlock()
	}
}

func garbageCollectionWorker() {
	ticker := time.NewTicker(24 * time.Hour) // Le GC peut √™tre espac√©
	defer ticker.Stop()
	for range ticker.C {
		runGarbageCollection()
	}
}

func findNewOffset(oldOffset uint64, offsetMap map[uint64]uint64) (uint64, bool) {
	if offsetMap == nil || len(offsetMap) == 0 {
		return 0, false
	}

	var bestMatchOldStart uint64
	found := false

	// Trouver le d√©but du bloc de donn√©es dans lequel notre chunk se trouvait
	// C'est le plus grand offset de d√©part dans la map qui est inf√©rieur ou √©gal √† notre offset
	for oldStart := range offsetMap {
		if oldStart <= oldOffset {
			if !found || oldStart > bestMatchOldStart {
				bestMatchOldStart = oldStart
				found = true
			}
		}
	}

	if found {
		newStart := offsetMap[bestMatchOldStart]
		relativeOffset := oldOffset - bestMatchOldStart
		return newStart + relativeOffset, true
	}

	return 0, false
}

// La fonction de garbage collection est maintenant transactionnelle gr√¢ce au journal
func runGarbageCollection() {
	log.Println("Lancement du garbage collection...")

	// √âtape 1: Nettoyer les fichiers marqu√©s comme supprim√©s de l'index
	deletedCount := 0
	state.Lock()
	filesToDelete := make([]string, 0)
	for filename, meta := range state.FileIndex {
		if meta.Deleted {
			filesToDelete = append(filesToDelete, filename)
		}
	}

	for _, filename := range filesToDelete {
		delete(state.FileIndex, filename)
		deletedCount++
	}
	state.Unlock()

	if deletedCount > 0 {
		log.Printf("Garbage collection: %d fichiers supprim√©s de l'index", deletedCount)
	}

	// √âtape 2: Envoyer les commandes de compactage aux volumes
	state.RLock()
	var onlineVolumes []*Volume
	for _, v := range state.RegisteredVolumes {
		if v.Status == "En ligne" {
			onlineVolumes = append(onlineVolumes, v)
		}
	}
	state.RUnlock()

	var wg sync.WaitGroup
	// Channel pour collecter les maps d'offsets des volumes
	offsetMapsChan := make(chan struct {
		VolumeName string
		OffsetMap  map[uint64]uint64
	}, len(onlineVolumes))

	for _, vol := range onlineVolumes {
		wg.Add(1)
		go func(volume *Volume) {
			defer wg.Done()
			compactURL := fmt.Sprintf("http://%s/compact", volume.Address)
			// Le compactage peut prendre du temps, on augmente le timeout
			client := &http.Client{Timeout: 15 * time.Minute}
			resp, err := client.Post(compactURL, "application/json", nil)
			if err != nil {
				log.Printf("Erreur lors de la demande de compactage au volume '%s': %v", volume.Name, err)
				return
			}
			defer resp.Body.Close()

			if resp.StatusCode == http.StatusOK {
				var compactResp struct {
					Status    string            `json:"status"`
					OffsetMap map[string]uint64 `json:"offset_map"`
				}
				if err := json.NewDecoder(resp.Body).Decode(&compactResp); err != nil {
					log.Printf("Erreur de d√©codage de la r√©ponse de compactage de '%s': %v", volume.Name, err)
					return
				}

				if compactResp.OffsetMap != nil && len(compactResp.OffsetMap) > 0 {
					// Convertir la map de string en map[uint64]uint64
					newOffsetMap := make(map[uint64]uint64)
					for oldOffStr, newOff := range compactResp.OffsetMap {
						oldOff, err := strconv.ParseUint(oldOffStr, 10, 64)
						if err == nil {
							newOffsetMap[oldOff] = newOff
						}
					}
					offsetMapsChan <- struct {
						VolumeName string
						OffsetMap  map[uint64]uint64
					}{VolumeName: volume.Name, OffsetMap: newOffsetMap}
				}
				log.Printf("Compactage du volume '%s' termin√© avec succ√®s", volume.Name)
			} else {
				body, _ := io.ReadAll(resp.Body)
				log.Printf("Erreur lors du compactage du volume '%s': status %d, body: %s", volume.Name, resp.StatusCode, string(body))
			}
		}(vol)
	}
	wg.Wait()
	close(offsetMapsChan)

	// √âtape 3: Collecter tous les r√©sultats et pr√©parer le journal
	journalData := GCJournal{
		VolumeOffsetMaps: make(map[string]map[uint64]uint64),
	}
	hasChanges := false
	for res := range offsetMapsChan {
		if len(res.OffsetMap) > 0 {
			journalData.VolumeOffsetMaps[res.VolumeName] = res.OffsetMap
			hasChanges = true
		}
	}

	if !hasChanges {
		log.Println("Garbage collection termin√©. Aucun changement d'offset √† appliquer.")
		return
	}

	// √âtape 4: √âcrire les changements dans un journal AVANT de modifier l'√©tat en m√©moire
	log.Println("√âcriture du journal de transaction pour le GC...")
	journalFile, err := os.Create(gcJournalPath)
	if err != nil {
		log.Printf("ERREUR CRITIQUE: Impossible de cr√©er le journal de GC: %v. Abandon du GC.", err)
		return
	}
	encoder := gob.NewEncoder(journalFile)
	if err := encoder.Encode(&journalData); err != nil {
		journalFile.Close()
		os.Remove(gcJournalPath)
		log.Printf("ERREUR CRITIQUE: Impossible d'√©crire dans le journal de GC: %v. Abandon.", err)
		return
	}
	// On s'assure que le journal est bien √©crit sur le disque physique
	if err := journalFile.Sync(); err != nil {
		journalFile.Close()
		os.Remove(gcJournalPath)
		log.Printf("ERREUR CRITIQUE: Impossible de synchroniser le journal de GC sur le disque: %v. Abandon.", err)
		return
	}
	journalFile.Close()
	log.Println("Journal de transaction du GC √©crit avec succ√®s.")

	// √âtape 5: Appliquer les changements √† l'index en m√©moire
	// (Le serveur crashe ici. Au red√©marrage, replayGCJournalIfExists() corrigera l'√©tat)
	applyOffsetMaps(&journalData)

	// √âtape 6: Sauvegarder l'index principal mis √† jour
	// (Le serveur crashe ici. Au red√©marrage, replayGCJournalIfExists() chargera l'ancien index, le corrigera et le sauvegardera)
	saveIndex()

	// √âtape 7: Supprimer le journal, l'op√©ration est termin√©e
	if err := os.Remove(gcJournalPath); err != nil {
		log.Printf("AVERTISSEMENT: Impossible de supprimer le journal de GC apr√®s succ√®s: %v", err)
	}

	log.Println("Garbage collection termin√© et index mis √† jour de mani√®re transactionnelle.")
}

// --- Handlers HTTP ---

func registerVolumeHandler(w http.ResponseWriter, r *http.Request) {
	var volData Volume
	if err := json.NewDecoder(r.Body).Decode(&volData); err != nil {
		http.Error(w, "JSON invalide: "+err.Error(), http.StatusBadRequest)
		return
	}
	if volData.Name == "" || volData.DiskID == "" {
		http.Error(w, "Le nom du volume et l'ID du disque sont requis.", http.StatusBadRequest)
		return
	}

	state.Lock()
	defer state.Unlock()

	existingVol, exists := state.RegisteredVolumes[volData.Name]

	updateVolume := func(v *Volume) {
		v.Address = volData.Address
		v.DiskID = volData.DiskID
		v.TotalSpace = volData.TotalSpace
		v.FreeSpace = volData.FreeSpace
		v.LastSeen = time.Now()
		v.Status = "En ligne"
	}

	if volData.Type == "initial" {
		if exists && existingVol.Status == "En ligne" {
			log.Printf("Conflit de nom de volume : Tentative d'enregistrement initial pour '%s' alors qu'il est d√©j√† en ligne.", volData.Name)
			http.Error(w, fmt.Sprintf("Impossible d'enregistrer le volume : un volume nomm√© '%s' est d√©j√† connect√©.", volData.Name), http.StatusConflict)
			return
		}
		if !exists {
			log.Printf("Nouveau volume enregistr√© : %s sur disque %s", volData.Name, volData.DiskID)
			existingVol = &Volume{Name: volData.Name}
			state.RegisteredVolumes[volData.Name] = existingVol
		} else {
			log.Printf("Le volume '%s' (pr√©c√©demment hors ligne) est de retour.", volData.Name)
		}
		updateVolume(existingVol)
		w.WriteHeader(http.StatusOK)
		return
	}

	if volData.Type == "heartbeat" {
		if !exists {
			log.Printf("Avertissement : Heartbeat re√ßu pour un volume inconnu '%s'. Enregistrement...", volData.Name)
			existingVol = &Volume{Name: volData.Name}
			state.RegisteredVolumes[volData.Name] = existingVol
		}
		updateVolume(existingVol)
		w.WriteHeader(http.StatusOK)
		return
	}

	http.Error(w, "Type de requ√™te ('initial' ou 'heartbeat') manquant.", http.StatusBadRequest)
}
func uploadFileHandler(w http.ResponseWriter, r *http.Request) {
	mr, err := r.MultipartReader()
	if err != nil {
		http.Error(w, "Erreur de lecture du formulaire multipart: "+err.Error(), http.StatusInternalServerError)
		return
	}
	part, err := mr.NextPart()
	if err != nil {
		http.Error(w, "Aucun fichier trouv√©: "+err.Error(), http.StatusBadRequest)
		return
	}
	fileName := part.FileName()
	if fileName == "" {
		http.Error(w, "Nom de fichier vide.", http.StatusBadRequest)
		return
	}
	log.Printf("D√©but de l'upload pour: %s", fileName)

	state.RLock()
	_, exists := state.FileIndex[fileName]
	state.RUnlock()
	if exists {
		http.Error(w, "Un fichier avec ce nom existe d√©j√†.", http.StatusConflict)
		return
	}

	var fileChunks []*ChunkMetadata
	var totalSize int64
	chunkIdx := 0
	var successfulChunks []string

	uploadValid := true
	var uploadError error

	for {
		buffer := make([]byte, chunkSize)
		bytesRead, readErr := io.ReadFull(part, buffer)

		isLastChunk := false
		if readErr == io.EOF {
			break
		}
		if readErr == io.ErrUnexpectedEOF {
			buffer = buffer[:bytesRead]
			isLastChunk = true
		} else if readErr != nil {
			go cleanupUploadChunks(successfulChunks)
			http.Error(w, "Erreur de lecture du chunk: "+readErr.Error(), http.StatusInternalServerError)
			return
		}

		hash := sha256.Sum256(buffer)
		checksum := hex.EncodeToString(hash[:])

		requiredSpace := uint64(len(buffer)) + minFreeSpaceForUpload
		targetVolumes, err := selectVolumesForReplication(requiredReplicas, nil, requiredSpace)
		if err != nil {
			go cleanupUploadChunks(successfulChunks)
			log.Printf("ERREUR UPLOAD: Impossible de trouver des volumes pour la r√©plication: %v", err)
			http.Error(w, "Erreur interne: "+err.Error(), http.StatusServiceUnavailable)
			return
		}

		log.Printf("Chunk %d (checksum: %s...): √©criture sur %s et %s",
			chunkIdx, checksum[:8], targetVolumes[0].Name, targetVolumes[1].Name)

		var wg sync.WaitGroup
		results := make(chan *ChunkCopy, requiredReplicas)
		errs := make(chan error, requiredReplicas)
		successChan := make(chan string, requiredReplicas)

		for _, vol := range targetVolumes {
			wg.Add(1)
			go func(v *Volume) {
				defer wg.Done()

				if !verifyVolumeOnline(v) {
					errs <- fmt.Errorf("volume '%s' devenu inaccessible avant √©criture", v.Name)
					return
				}

				diskURL := fmt.Sprintf("http://%s/write_chunk", v.Address)
				req, _ := http.NewRequest("POST", diskURL, bytes.NewReader(buffer))
				req.Header.Set("Content-Type", "application/octet-stream")
				req.Header.Set("X-Chunk-Checksum", checksum)

				client := &http.Client{Timeout: 30 * time.Second}
				resp, err := client.Do(req)
				if err != nil {
					log.Printf("ERREUR: Volume '%s' inaccessible, marquage hors ligne", v.Name)
					state.Lock()
					if vol, exists := state.RegisteredVolumes[v.Name]; exists {
						vol.Status = "Hors ligne"
					}
					state.Unlock()
					errs <- fmt.Errorf("volume '%s' injoignable: %w", v.Name, err)
					return
				}
				defer resp.Body.Close()

				if resp.StatusCode != http.StatusOK {
					body, _ := io.ReadAll(resp.Body)
					errs <- fmt.Errorf("le volume '%s' a refus√© l'√©criture (status: %d): %s", v.Name, resp.StatusCode, string(body))
					return
				}

				var writeResp struct {
					Offset uint64 `json:"offset"`
					Size   uint32 `json:"size"`
				}
				if err := json.NewDecoder(resp.Body).Decode(&writeResp); err != nil {
					errs <- fmt.Errorf("r√©ponse invalide du volume '%s'", v.Name)
					return
				}

				successChan <- fmt.Sprintf("%s:%s:%d:%d", v.Name, checksum, writeResp.Offset, writeResp.Size)
				results <- &ChunkCopy{VolumeName: v.Name, Offset: writeResp.Offset, Size: writeResp.Size}
			}(vol)
		}
		wg.Wait()
		close(results)
		close(errs)
		close(successChan)

		var successfulCopies []*ChunkCopy
		var partialSuccesses []string
		for res := range results {
			successfulCopies = append(successfulCopies, res)
		}
		for success := range successChan {
			partialSuccesses = append(partialSuccesses, success)
		}
		for e := range errs {
			log.Printf("ERREUR UPLOAD: %v", e)
		}

		if len(successfulCopies) < requiredReplicas {
			uploadValid = false
			uploadError = fmt.Errorf("chunk %d: seulement %d copies r√©ussies sur %d requises",
				chunkIdx, len(successfulCopies), requiredReplicas)
			break
		}

		uniqueDisks := make(map[string]bool)
		state.RLock()
		for _, copy := range successfulCopies {
			if vol, exists := state.RegisteredVolumes[copy.VolumeName]; exists {
				uniqueDisks[vol.DiskID] = true
			}
		}
		state.RUnlock()

		if len(uniqueDisks) < requiredReplicas {
			uploadValid = false
			uploadError = fmt.Errorf("chunk %d: copies sur seulement %d disques diff√©rents (requis: %d)",
				chunkIdx, len(uniqueDisks), requiredReplicas)
			break
		}

		successfulChunks = append(successfulChunks, partialSuccesses...)

		fileChunks = append(fileChunks, &ChunkMetadata{
			ChunkIdx: chunkIdx,
			Copies:   successfulCopies,
			Checksum: checksum,
		})
		totalSize += int64(successfulCopies[0].Size)
		chunkIdx++

		if isLastChunk {
			break
		}
	}

	if !uploadValid {
		log.Printf("ERREUR UPLOAD: Upload invalide pour '%s': %v", fileName, uploadError)
		go cleanupUploadChunks(successfulChunks)
		http.Error(w, "√âchec de l'upload: "+uploadError.Error(), http.StatusInternalServerError)
		return
	}

	tempMeta := &FileMetadata{
		FileName:   fileName,
		TotalSize:  totalSize,
		UploadDate: time.Now(),
		Chunks:     fileChunks,
	}

	state.Lock()
	state.FileIndex[fileName] = tempMeta
	state.Unlock()

	if err := validateUploadedFile(fileName); err != nil {
		log.Printf("ERREUR VALIDATION: Upload de '%s' √©choue √† la validation: %v", fileName, err)

		state.Lock()
		delete(state.FileIndex, fileName)
		state.Unlock()

		go cleanupUploadChunks(successfulChunks)
		http.Error(w, "√âchec de la validation: "+err.Error(), http.StatusInternalServerError)
		return
	}

	saveIndex()
	log.Printf("Fichier %s (taille: %d, chunks: %d) upload√© et valid√© avec succ√®s.", fileName, totalSize, len(fileChunks))
	http.Redirect(w, r, "/", http.StatusSeeOther)
}

func cleanupUploadChunks(chunkIdentifiers []string) {
	if len(chunkIdentifiers) == 0 {
		return
	}

	log.Printf("Nettoyage de %d chunks suite √† un √©chec d'upload", len(chunkIdentifiers))

	volumeChunks := make(map[string][]map[string]interface{})

	for _, identifier := range chunkIdentifiers {
		parts := parseChunkIdentifier(identifier)
		if len(parts) == 4 {
			volumeName := parts[0]
			checksum := parts[1]
			offset := parts[2]
			size := parts[3]

			volumeChunks[volumeName] = append(volumeChunks[volumeName], map[string]interface{}{
				"checksum": checksum,
				"offset":   offset,
				"size":     size,
			})
		}
	}

	state.RLock()
	volumes := make(map[string]*Volume)
	for name, vol := range state.RegisteredVolumes {
		if vol.Status == "En ligne" {
			volumes[name] = vol
		}
	}
	state.RUnlock()

	for volumeName, chunks := range volumeChunks {
		if vol, ok := volumes[volumeName]; ok {
			if !verifyVolumeOnline(vol) {
				log.Printf("S√âCURIT√â: Volume '%s' inaccessible, impossible de nettoyer les chunks orphelins", volumeName)
				continue
			}

			cleanupURL := fmt.Sprintf("http://%s/cleanup_orphan", vol.Address)
			cleanupData := map[string]interface{}{
				"chunks": chunks,
			}

			jsonData, _ := json.Marshal(cleanupData)
			client := &http.Client{Timeout: 10 * time.Second}
			req, _ := http.NewRequest("POST", cleanupURL, bytes.NewReader(jsonData))
			req.Header.Set("Content-Type", "application/json")

			resp, err := client.Do(req)
			if err != nil {
				log.Printf("Erreur lors du nettoyage des chunks orphelins sur '%s': %v", volumeName, err)
				state.Lock()
				if volume, exists := state.RegisteredVolumes[volumeName]; exists {
					volume.Status = "Hors ligne"
				}
				state.Unlock()
			} else {
				resp.Body.Close()
				log.Printf("Chunks orphelins nettoy√©s sur le volume '%s'", volumeName)
			}
		}
	}
}

func parseChunkIdentifier(identifier string) []string {
	parts := make([]string, 0, 4)
	current := ""
	colonCount := 0

	for _, char := range identifier {
		if char == ':' && colonCount < 3 {
			parts = append(parts, current)
			current = ""
			colonCount++
		} else {
			current += string(char)
		}
	}
	if current != "" {
		parts = append(parts, current)
	}

	return parts
}

func downloadFileHandler(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	filename := vars["filename"]
	state.RLock()
	meta, ok := state.FileIndex[filename]
	if !ok || meta.Deleted {
		state.RUnlock()
		http.NotFound(w, r)
		return
	}
	chunks := make([]*ChunkMetadata, len(meta.Chunks))
	copy(chunks, meta.Chunks)
	state.RUnlock()

	w.Header().Set("Content-Disposition", "attachment; filename=\""+filename+"\"")
	w.Header().Set("Content-Type", "application/octet-stream")
	w.Header().Set("Content-Length", fmt.Sprintf("%d", meta.TotalSize))

	for _, chunk := range chunks {
		var readSuccessful bool
		var lastError error
		expectedChecksum := chunk.Checksum

		for _, copyInfo := range chunk.Copies {
			if copyInfo.Deleted {
				continue
			}

			state.RLock()
			volume, volOK := state.RegisteredVolumes[copyInfo.VolumeName]
			state.RUnlock()

			if !volOK {
				lastError = fmt.Errorf("volume '%s' non trouv√©", copyInfo.VolumeName)
				continue
			}

			if !verifyVolumeOnline(volume) {
				lastError = fmt.Errorf("volume '%s' hors ligne (v√©rification temps r√©el)", copyInfo.VolumeName)
				continue
			}

			diskURL := fmt.Sprintf("http://%s/read_chunk?offset=%d&size=%d", volume.Address, copyInfo.Offset, copyInfo.Size)

			client := &http.Client{Timeout: 10 * time.Second}
			resp, err := client.Get(diskURL)
			if err != nil {
				log.Printf("S√âCURIT√â: Erreur de lecture d√©tect√©e sur '%s', marquage hors ligne", volume.Name)
				state.Lock()
				if vol, exists := state.RegisteredVolumes[copyInfo.VolumeName]; exists {
					vol.Status = "Hors ligne"
				}
				state.Unlock()
				lastError = fmt.Errorf("erreur de connexion √† '%s': %w", volume.Name, err)
				continue
			}

			if resp.StatusCode != http.StatusOK {
				bodyBytes, _ := io.ReadAll(resp.Body)
				resp.Body.Close()
				lastError = fmt.Errorf("√©chec de lecture sur '%s' (status: %d): %s", volume.Name, resp.StatusCode, string(bodyBytes))
				continue
			}

			chunkData, err := io.ReadAll(resp.Body)
			resp.Body.Close()
			if err != nil {
				lastError = fmt.Errorf("erreur de lecture du corps de la r√©ponse de '%s': %w", volume.Name, err)
				continue
			}

			hash := sha256.Sum256(chunkData)
			actualChecksum := hex.EncodeToString(hash[:])

			if actualChecksum != expectedChecksum {
				log.Printf("!!! CORRUPTION DETECTEE !!! Chunk %d de '%s' sur volume '%s'. Attendu: %s, Re√ßu: %s",
					chunk.ChunkIdx, filename, volume.Name, expectedChecksum, actualChecksum)
				lastError = fmt.Errorf("corruption de donn√©es sur '%s'", volume.Name)
				continue
			}

			if _, err := w.Write(chunkData); err != nil {
				log.Printf("Erreur d'envoi du chunk au client: %v", err)
				return
			}

			readSuccessful = true
			break
		}

		if !readSuccessful {
			log.Printf("√âchec du t√©l√©chargement pour '%s': aucune copie valide du chunk %d n'est disponible. Derni√®re erreur: %v", filename, chunk.ChunkIdx, lastError)
			http.Error(w, fmt.Sprintf("Fichier corrompu ou indisponible, impossible de lire le chunk %d", chunk.ChunkIdx), http.StatusServiceUnavailable)
			return
		}
	}
}

func deleteFileHandler(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	filename := vars["filename"]

	state.RLock()
	meta, exists := state.FileIndex[filename]
	if !exists {
		state.RUnlock()
		http.NotFound(w, r)
		return
	}

	var chunksToDelete []struct {
		volumeName string
		offset     uint64
		size       uint32
	}

	for _, chunk := range meta.Chunks {
		for _, copy := range chunk.Copies {
			chunksToDelete = append(chunksToDelete, struct {
				volumeName string
				offset     uint64
				size       uint32
			}{copy.VolumeName, copy.Offset, copy.Size})
		}
	}
	state.RUnlock()

	state.Lock()
	if meta, exists := state.FileIndex[filename]; exists {
		meta.Deleted = true
		for _, chunk := range meta.Chunks {
			for _, copy := range chunk.Copies {
				copy.Deleted = true
			}
		}
	}
	state.Unlock()

	saveIndex()
	log.Printf("Fichier '%s' marqu√© pour suppression", filename)

	go func() {
		volumeChunks := make(map[string][]map[string]interface{})

		for _, chunk := range chunksToDelete {
			volumeChunks[chunk.volumeName] = append(volumeChunks[chunk.volumeName], map[string]interface{}{
				"offset": chunk.offset,
				"size":   chunk.size,
			})
		}

		state.RLock()
		volumes := make(map[string]*Volume)
		for name, vol := range state.RegisteredVolumes {
			if vol.Status == "En ligne" {
				volumes[name] = vol
			}
		}
		state.RUnlock()

		for volumeName, chunks := range volumeChunks {
			if vol, ok := volumes[volumeName]; ok {
				deleteURL := fmt.Sprintf("http://%s/mark_deleted", vol.Address)
				deleteData := map[string]interface{}{
					"chunks": chunks,
				}

				jsonData, _ := json.Marshal(deleteData)
				req, _ := http.NewRequest("POST", deleteURL, bytes.NewReader(jsonData))
				req.Header.Set("Content-Type", "application/json")

				client := &http.Client{Timeout: 5 * time.Second}
				resp, err := client.Do(req)
				if err != nil {
					log.Printf("Erreur lors de la notification de suppression au volume '%s': %v", volumeName, err)
				} else {
					resp.Body.Close()
					log.Printf("Chunks du fichier '%s' marqu√©s pour suppression sur le volume '%s'", filename, volumeName)
				}
			}
		}
	}()

	http.Redirect(w, r, "/", http.StatusSeeOther)
}

func repairHandler(w http.ResponseWriter, r *http.Request) {
	log.Println("Lancement de la proc√©dure de r√©paration en arri√®re-plan...")
	go runRepairProcess()
	http.Redirect(w, r, "/", http.StatusSeeOther)
}

func garbageCollectionHandler(w http.ResponseWriter, r *http.Request) {
	log.Println("Lancement du garbage collection en arri√®re-plan...")
	go runGarbageCollection()
	http.Redirect(w, r, "/", http.StatusSeeOther)
}

// NOUVEAU: Handler pour nettoyer les copies en surplus
func cleanupReplicasHandler(w http.ResponseWriter, r *http.Request) {
	log.Println("Lancement du nettoyage des r√©pliques exc√©dentaires en arri√®re-plan...")
	go runCleanupReplicasProcess()
	http.Redirect(w, r, "/", http.StatusSeeOther)
}

// NOUVEAU: Processus de nettoyage des copies en surplus
func runCleanupReplicasProcess() {
	log.Println("D√©but du processus de nettoyage des r√©pliques exc√©dentaires.")

	volumeDeletionMap := make(map[string][]map[string]interface{})
	cleanedCount := 0

	state.Lock()

	filesToProcess := make([]*FileMetadata, 0, len(state.FileIndex))
	for _, meta := range state.FileIndex {
		if !meta.Deleted {
			filesToProcess = append(filesToProcess, meta)
		}
	}

	onlineVolumes := make(map[string]*Volume)
	for name, vol := range state.RegisteredVolumes {
		if vol.Status == "En ligne" {
			onlineVolumes[name] = vol
		}
	}

	for _, meta := range filesToProcess {
		for chunkIdx, chunk := range meta.Chunks {

			var onlineCopies []*ChunkCopy
			for _, copyInfo := range chunk.Copies {
				if !copyInfo.Deleted {
					if _, ok := onlineVolumes[copyInfo.VolumeName]; ok {
						onlineCopies = append(onlineCopies, copyInfo)
					}
				}
			}

			extrasToRemove := len(onlineCopies) - requiredReplicas

			if extrasToRemove > 0 {
				rand.Shuffle(len(onlineCopies), func(i, j int) {
					onlineCopies[i], onlineCopies[j] = onlineCopies[j], onlineCopies[i]
				})

				copiesToDelete := onlineCopies[:extrasToRemove]

				for _, copyToDelete := range copiesToDelete {
					copyToDelete.Deleted = true

					volumeDeletionMap[copyToDelete.VolumeName] = append(volumeDeletionMap[copyToDelete.VolumeName], map[string]interface{}{
						"offset": copyToDelete.Offset,
						"size":   copyToDelete.Size,
					})
					cleanedCount++
					log.Printf("Nettoyage: Marqu√© la copie du chunk %d du fichier '%s' sur le volume '%s' pour suppression.", chunkIdx, meta.FileName, copyToDelete.VolumeName)
				}
			}
		}
	}

	state.Unlock()

	if cleanedCount > 0 {
		saveIndex()

		go func() {
			for volumeName, chunks := range volumeDeletionMap {
				if vol, ok := onlineVolumes[volumeName]; ok {
					deleteURL := fmt.Sprintf("http://%s/mark_deleted", vol.Address)
					deleteData := map[string]interface{}{"chunks": chunks}

					jsonData, _ := json.Marshal(deleteData)
					req, _ := http.NewRequest("POST", deleteURL, bytes.NewReader(jsonData))
					req.Header.Set("Content-Type", "application/json")

					client := &http.Client{Timeout: 5 * time.Second}
					resp, err := client.Do(req)
					if err != nil {
						log.Printf("Erreur lors de la notification de suppression (nettoyage) au volume '%s': %v", volumeName, err)
					} else {
						resp.Body.Close()
						log.Printf("Copies exc√©dentaires marqu√©es pour suppression sur le volume '%s'", volumeName)
					}
				}
			}
		}()
		log.Printf("Nettoyage des r√©pliques termin√©. %d copies ont √©t√© marqu√©es pour suppression.", cleanedCount)
	} else {
		log.Println("Nettoyage des r√©pliques termin√©. Aucune copie exc√©dentaire n'a √©t√© trouv√©e.")
	}
}


func runRepairProcess() {
	type repairJob struct {
		filename      string
		chunkIdx      int
		chunkChecksum string
		sourceCopy    *ChunkCopy
		sourceVolume  *Volume
	}
	var jobs []repairJob

	state.RLock()
	for filename, meta := range state.FileIndex {
		if meta.Deleted {
			continue
		}
		for _, chunk := range meta.Chunks {
			var onlineCopies []*ChunkCopy
			for _, copyInfo := range chunk.Copies {
				if !copyInfo.Deleted {
					if vol, ok := state.RegisteredVolumes[copyInfo.VolumeName]; ok && vol.Status == "En ligne" {
						onlineCopies = append(onlineCopies, copyInfo)
					}
				}
			}

			if len(onlineCopies) > 0 && len(onlineCopies) < requiredReplicas {
				sourceCopy := onlineCopies[0]
				sourceVolume := state.RegisteredVolumes[sourceCopy.VolumeName]
				jobs = append(jobs, repairJob{filename, chunk.ChunkIdx, chunk.Checksum, sourceCopy, sourceVolume})
			}
		}
	}
	state.RUnlock()

	if len(jobs) == 0 {
		log.Println("Proc√©dure de r√©paration termin√©e. Aucun chunk d√©grad√© et r√©parable n'a √©t√© trouv√©.")
		return
	}

	log.Printf("D√©but de la r√©paration pour %d chunks...", len(jobs))
	repairedChunks := 0

	for _, job := range jobs {
		excludeDiskIDs := []string{}
		state.RLock()
		if meta, ok := state.FileIndex[job.filename]; ok && job.chunkIdx < len(meta.Chunks) {
			for _, c := range meta.Chunks[job.chunkIdx].Copies {
				if !c.Deleted {
					if v, ok := state.RegisteredVolumes[c.VolumeName]; ok && v.Status == "En ligne" {
						excludeDiskIDs = append(excludeDiskIDs, v.DiskID)
					}
				}
			}
		} else {
			state.RUnlock()
			continue
		}

		requiredSpace := uint64(job.sourceCopy.Size) + minFreeSpaceForUpload
		targetVolumes, err := selectVolumesForReplication(1, excludeDiskIDs, requiredSpace)
		state.RUnlock()

		if err != nil {
			log.Printf("ERREUR REPARATION: Impossible de trouver un volume de destination pour chunk %d de '%s': %v", job.chunkIdx, job.filename, err)
			continue
		}
		targetVolume := targetVolumes[0]

		readURL := fmt.Sprintf("http://%s/read_chunk?offset=%d&size=%d", job.sourceVolume.Address, job.sourceCopy.Offset, job.sourceCopy.Size)
		resp, err := http.Get(readURL)
		if err != nil || resp.StatusCode != http.StatusOK {
			log.Printf("ERREUR REPARATION: Impossible de lire le chunk source depuis '%s': %v", job.sourceVolume.Name, err)
			if resp != nil {
				resp.Body.Close()
			}
			continue
		}
		chunkData, err := io.ReadAll(resp.Body)
		resp.Body.Close()
		if err != nil {
			log.Printf("ERREUR REPARATION: Impossible de lire le corps de la r√©ponse de '%s': %v", job.sourceVolume.Name, err)
			continue
		}

		hash := sha256.Sum256(chunkData)
		actualChecksum := hex.EncodeToString(hash[:])
		if actualChecksum != job.chunkChecksum {
			log.Printf("!!! CORRUPTION DETECTEE PENDANT REPARATION !!! La source du chunk %d ('%s' sur '%s') est corrompue. Annulation de la copie.",
				job.chunkIdx, job.filename, job.sourceVolume.Name)
			continue
		}

		writeURL := fmt.Sprintf("http://%s/write_chunk", targetVolume.Address)
		req, _ := http.NewRequest("POST", writeURL, bytes.NewReader(chunkData))
		req.Header.Set("Content-Type", "application/octet-stream")
		req.Header.Set("X-Chunk-Checksum", job.chunkChecksum)
		writeResp, err := http.DefaultClient.Do(req)
		if err != nil || writeResp.StatusCode != http.StatusOK {
			log.Printf("ERREUR REPARATION: Impossible d'√©crire le chunk sur la destination '%s': %v", targetVolume.Name, err)
			if writeResp != nil {
				writeResp.Body.Close()
			}
			continue
		}

		var writeRespBody struct {
			Offset uint64 `json:"offset"`
			Size   uint32 `json:"size"`
		}
		if err := json.NewDecoder(writeResp.Body).Decode(&writeRespBody); err != nil {
			writeResp.Body.Close()
			log.Printf("ERREUR REPARATION: R√©ponse JSON invalide de '%s'", targetVolume.Name)
			continue
		}
		writeResp.Body.Close()

		state.Lock()
		newCopy := &ChunkCopy{
			VolumeName: targetVolume.Name,
			Offset:     writeRespBody.Offset,
			Size:       writeRespBody.Size,
		}
		if meta, ok := state.FileIndex[job.filename]; ok && job.chunkIdx < len(meta.Chunks) {
			meta.Chunks[job.chunkIdx].Copies = append(meta.Chunks[job.chunkIdx].Copies, newCopy)
			repairedChunks++
			log.Printf("Succ√®s: Chunk %d de '%s' recopi√© de '%s' vers '%s' (Disque: %s)",
				job.chunkIdx, job.filename, job.sourceVolume.Name, targetVolume.Name, targetVolume.DiskID)
		}
		state.Unlock()
	}

	if repairedChunks > 0 {
		saveIndex()
		log.Printf("Proc√©dure de r√©paration termin√©e. %d chunks ont √©t√© restaur√©s.", repairedChunks)
	} else {
		log.Println("Proc√©dure de r√©paration termin√©e. Aucun chunk n'a pu √™tre r√©par√©.")
	}
}

// MODIFI√â: Ajout de la variable HasOverReplicatedFiles
func webUIHandler(w http.ResponseWriter, r *http.Request) {
	state.RLock()
	defer state.RUnlock()

	volumes := make([]*Volume, 0, len(state.RegisteredVolumes))
	onlinePhysicalDisks := make(map[string]bool)
	for _, v := range state.RegisteredVolumes {
		volumes = append(volumes, v)
		if v.Status == "En ligne" {
			onlinePhysicalDisks[v.DiskID] = true
		}
	}

	files := make([]*FileMetadata, 0, len(state.FileIndex))
	filesInDegradedState := false
	hasOverReplicatedFiles := false // Flag pour les fichiers sur-prot√©g√©s
	isRepairPossible := false
	deletedFilesCount := 0

	for _, f := range state.FileIndex {
		if f.Deleted {
			deletedFilesCount++
			continue
		}

		f.Status = calculateFileStatus(f)

		if f.Status == "D√©grad√©" {
			filesInDegradedState = true

			for _, chunk := range f.Chunks {
				onlineCopyDisks := make(map[string]bool)
				for _, copyInfo := range chunk.Copies {
					if !copyInfo.Deleted {
						if vol, ok := state.RegisteredVolumes[copyInfo.VolumeName]; ok && vol.Status == "En ligne" {
							onlineCopyDisks[vol.DiskID] = true
						}
					}
				}
				if len(onlinePhysicalDisks) > len(onlineCopyDisks) {
					isRepairPossible = true
					break
				}
			}
		}

		if f.Status == "Sur-prot√©g√©" {
			hasOverReplicatedFiles = true
		}

		files = append(files, f)
	}

	notEnoughDisksForRedundancy := len(onlinePhysicalDisks) < requiredReplicas

	sort.Slice(volumes, func(i, j int) bool {
		if volumes[i].DiskID == volumes[j].DiskID {
			return volumes[i].Name < volumes[j].Name
		}
		return volumes[i].DiskID < volumes[j].DiskID
	})
	sort.Slice(files, func(i, j int) bool { return files[i].UploadDate.After(files[j].UploadDate) })

	data := struct {
		Volumes                     []*Volume
		Files                       []*FileMetadata
		FilesInDegradedState        bool
		HasOverReplicatedFiles      bool // Variable pour le template
		NotEnoughDisksForRedundancy bool
		RequiredReplicas            int
		CanRepair                   bool
		DeletedFilesCount           int
	}{
		Volumes:                     volumes,
		Files:                       files,
		FilesInDegradedState:        filesInDegradedState,
		HasOverReplicatedFiles:      hasOverReplicatedFiles,
		NotEnoughDisksForRedundancy: notEnoughDisksForRedundancy,
		RequiredReplicas:            requiredReplicas,
		CanRepair:                   isRepairPossible,
		DeletedFilesCount:           deletedFilesCount,
	}

	err := webTemplate.Execute(w, data)
	if err != nil {
		http.Error(w, err.Error(), http.StatusInternalServerError)
	}
}

// MODIFI√â: Mise √† jour du template HTML
const htmlTemplate = `
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8"><title>Stockage Distribu√©</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; background-color: #f4f7f6; color: #333; margin: 2em; }
        .container { max-width: 1200px; margin: auto; background: white; padding: 2em; border-radius: 8px; box-shadow: 0 4px 15px rgba(0,0,0,0.05); }
        h1, h2 { color: #2c3e50; border-bottom: 2px solid #e0e0e0; padding-bottom: 0.5em;}
		.alert { padding: 1em; margin-bottom: 1em; border-radius: 5px; display: flex; justify-content: space-between; align-items: center; }
		.alert-warning { background-color: #fcf8e3; border: 1px solid #faebcc; color: #8a6d3b; }
        .alert-danger { background-color: #f2dede; border: 1px solid #ebccd1; color: #a94442; }
        .alert-info { background-color: #d9edf7; border: 1px solid #bce8f1; color: #31708f; }
        .grid { display: grid; grid-template-columns: 1fr 2fr; gap: 2em; }
        table { width: 100%; border-collapse: collapse; margin-top: 1em; }
        th, td { text-align: left; padding: 12px; border-bottom: 1px solid #ddd; }
        th { background-color: #f2f2f2; }
        .upload-form { background: #f9f9f9; padding: 1.5em; border-radius: 5px; border: 1px solid #ddd; }
        .btn { background-color: #3498db; color: white; padding: 10px 15px; border: none; border-radius: 4px; cursor: pointer; text-decoration: none; display: inline-block; font-size: 14px; margin-right: 5px; }
        .btn-download { background-color: #27ae60; }
        .btn-delete { background-color: #e74c3c; }
        .btn-repair { background-color: #e67e22; }
        .btn-gc { background-color: #9b59b6; }
		.btn-disabled { background-color: #bdc3c7; cursor: not-allowed; }
        .status-online { color: #27ae60; font-weight: bold; }
        .status-offline { color: #c0392b; font-weight: bold; }
        .status-protected { color: #27ae60; }
        .status-degraded { color: #f39c12; }
        .status-unavailable { color: #c0392b; font-weight: bold; }
		.status-over-protected { color: #3498db; }
        .volume-offline td { color: #95a5a6; }
        .action-buttons { display: flex; gap: 5px; }
        .confirm-delete { background-color: #c0392b; }
    </style>
    <script>
        function confirmDelete(filename) {
            return confirm('√ätes-vous s√ªr de vouloir supprimer le fichier "' + filename + '" ?\n\nCette action est irr√©versible.');
        }
    </script>
</head>
<body>
    <div class="container">
        <h1>üíø Panneau de Contr√¥le du Stockage</h1>
        
        {{if .NotEnoughDisksForRedundancy}}
        <div class="alert alert-danger">
            <strong>Attention !</strong> Pas assez de disques physiques en ligne pour garantir la duplication ({{.RequiredReplicas}} requis). Les nouveaux uploads √©choueront.
        </div>
        {{end}}
        {{if .FilesInDegradedState}}
        <div class="alert alert-warning">
			<div>
				<strong>Redondance compromise !</strong> Certains fichiers sont d√©grad√©s car un disque est hors ligne.
				<br>Les donn√©es sont toujours accessibles mais ne sont plus dupliqu√©es.
			</div>
			<div>
				<form action="/api/repair" method="post" style="margin:0;">
					<button type="submit" class="btn btn-repair" {{if not .CanRepair}}disabled title="Pas assez de disques diff√©rents en ligne pour la r√©paration"{{end}}>
						Effectuer une r√©paration
					</button>
				</form>
			</div>
        </div>
        {{end}}
		{{if .HasOverReplicatedFiles}}
        <div class="alert alert-info">
			<div>
				<strong>Optimisation possible !</strong> Certains fichiers ont plus de copies que n√©cessaire ({{.RequiredReplicas}} requises).
				<br>Vous pouvez lib√©rer de l'espace en supprimant les copies exc√©dentaires.
			</div>
			<div>
				<form action="/api/cleanup_replicas" method="post" style="margin:0;">
					<button type="submit" class="btn">
						Nettoyer les copies en surplus
					</button>
				</form>
			</div>
        </div>
        {{end}}
        {{if gt .DeletedFilesCount 0}}
        <div class="alert alert-info">
			<div>
				<strong>Nettoyage disponible !</strong> {{.DeletedFilesCount}} fichier(s) supprim√©(s) en attente de nettoyage.
				<br>Lancez le garbage collection pour lib√©rer d√©finitivement l'espace disque.
			</div>
			<div>
				<form action="/api/gc" method="post" style="margin:0;">
					<button type="submit" class="btn btn-gc">
						Lancer le nettoyage
					</button>
				</form>
			</div>
        </div>
        {{end}}

        <div class="grid">
            <div>
                <h2>Volumes ({{len .Volumes}})</h2>
                <table>
                    <thead><tr><th>Disque Physique</th><th>Nom Volume</th><th>Espace Libre</th><th>Statut</th></tr></thead>
                    <tbody>
                    {{range .Volumes}}
                        <tr class="{{if eq .Status "Hors ligne"}}volume-offline{{end}}">
                            <td>{{.DiskID}}</td>
                            <td>{{.Name}}</td>
                            <td>{{.FreeSpaceGB | printf "%.2f"}} GB</td>
                            <td>
                                {{if eq .Status "En ligne"}}
                                    <span class="status-online">‚óè En ligne</span>
                                {{else}}
                                    <span class="status-offline">‚óè Hors ligne</span>
                                {{end}}
                            </td>
                        </tr>
                    {{else}}
                        <tr><td colspan="4">Aucun volume enregistr√©.</td></tr>
                    {{end}}
                    </tbody>
                </table>
            </div>
            <div>
                <h2>Fichiers Stock√©s ({{len .Files}})</h2>
                <div class="upload-form">
                    <h3>Ajouter un nouveau fichier</h3>
                    <form action="/api/files/upload" method="post" enctype="multipart/form-data">
                        <input type="file" name="file" required>
                        <button type="submit" class="btn" {{if .NotEnoughDisksForRedundancy}}disabled title="Pas assez de disques pour l'upload"{{end}}>Envoyer</button>
                    </form>
                </div>
                <table>
                    <thead><tr><th>Nom</th><th>Taille</th><th>Chunks</th><th>Date</th><th>Statut de Redondance</th><th>Actions</th></tr></thead>
                    <tbody>
                    {{range .Files}}
                        <tr>
                            <td>{{.FileName}}</td>
                            <td>{{.TotalSizeMB | printf "%.2f"}} MB</td>
                            <td>{{len .Chunks}}</td>
                            <td>{{.UploadDate.Format "02/01/2006 15:04"}}</td>
                            <td>
                                {{if eq .Status "Prot√©g√©"}} <span class="status-protected">‚úî Prot√©g√©</span>
                                {{else if eq .Status "D√©grad√©"}} <span class="status-degraded">‚ö† D√©grad√©</span>
								{{else if eq .Status "Sur-prot√©g√©"}} <span class="status-over-protected">‚ÑπÔ∏è Sur-prot√©g√©</span>
                                {{else}} <span class="status-unavailable">‚úñ Indisponible</span>
                                {{end}}
                            </td>
                            <td>
                                <div class="action-buttons">
                                    <a href="/api/files/download/{{.FileName}}" 
                                       class="btn btn-download {{if eq .Status "Indisponible"}}btn-disabled{{end}}"
                                       {{if eq .Status "Indisponible"}}onclick="return false;"{{end}}>
                                       T√©l√©charger
                                    </a>
                                    <form action="/api/files/delete/{{.FileName}}" method="post" style="display:inline;">
                                        <button type="submit" class="btn btn-delete" onclick="return confirmDelete('{{.FileName}}')">
                                            Supprimer
                                        </button>
                                    </form>
                                </div>
                            </td>
                        </tr>
                    {{else}}
                        <tr><td colspan="6">Aucun fichier stock√©.</td></tr>
                    {{end}}
                    </tbody>
                </table>
            </div>
        </div>
    </div>
</body>
</html>
`
